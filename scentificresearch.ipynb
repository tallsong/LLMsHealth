{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca04959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bibtexparser # Used to read .bib files\n",
    "import re\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ieee_df = pd.read_csv(\"dataset/IEEE.csv\")\n",
    "ieee_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/acm.bib\", encoding=\"utf-8\") as bibtex_file:\n",
    "    bib_database = bibtexparser.load(bibtex_file)\n",
    "\n",
    "# Convert entries to a pandas DataFrame\n",
    "acm_df = pd.DataFrame(bib_database.entries)\n",
    "acm_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "springer_df = pd.read_parquet(\"dataset/springer.parquet\")\n",
    "springer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pubmed parquet\n",
    "pubmed_df = pd.read_parquet(\"dataset/pubmed.parquet\")\n",
    "\n",
    "pubmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciencedirect_df = pd.read_parquet(\"dataset/sciencedirect.parquet\")\n",
    "elsevier_df = sciencedirect_df\n",
    "elsevier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acefcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a list of tuples: (dataframe_name, title_series)\n",
    "df_sources = [\n",
    "    (\"ACM\", acm_df[\"title\"]),\n",
    "    (\"Elsevier\", elsevier_df[\"title\"]),\n",
    "    (\"IEEE\", ieee_df[\"Document Title\"]),\n",
    "    (\"PubMed\", pubmed_df[\"Title\"]),\n",
    "    (\"Springer\", springer_df[\"Item Title\"])\n",
    "]\n",
    "\n",
    "# 2. Create a combined DataFrame with source information\n",
    "combined_data = []\n",
    "for source_name, title_series in df_sources:\n",
    "    # Create a temporary DataFrame with title and source\n",
    "    temp_df = pd.DataFrame({\n",
    "        'title': title_series.str.lower(),  # Normalize to lowercase\n",
    "        'source': source_name\n",
    "    })\n",
    "    combined_data.append(temp_df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "all_titles_df = pd.concat(combined_data, ignore_index=True)\n",
    "print(f\"Amount of papers direct (from all libraries) directly after downloading / boolean queries:: {len(all_titles_df)}\")\n",
    "\n",
    "# 3. Remove rows with missing titles\n",
    "all_titles_df = all_titles_df.dropna(subset=['title'])\n",
    "\n",
    "# 4. Find titles that appear more than once\n",
    "title_counts = all_titles_df['title'].value_counts()\n",
    "duplicate_titles = title_counts[title_counts > 1].index.tolist()\n",
    "\n",
    "# 5. Filter to show only duplicates and group by title\n",
    "if not duplicate_titles:\n",
    "    print(\"No duplicate titles found across the DataFrames.\")\n",
    "else:\n",
    "    duplicates_df = all_titles_df[all_titles_df['title'].isin(duplicate_titles)]\n",
    "    \n",
    "    print(f\"Found {len(duplicate_titles)} duplicate titles across datasets\")\n",
    "    print(f\"Total duplicate entries: {len(duplicates_df)}\")\n",
    "    print(\"=\" * 80)\n",
    "    # Group by title and show which sources contain each duplicate\n",
    "    for title in sorted(duplicate_titles):\n",
    "        title_data = duplicates_df[duplicates_df['title'] == title]\n",
    "        sources = title_data['source'].tolist()\n",
    "        count = len(sources)\n",
    "\n",
    "        \n",
    "        print(f\"\\nTitle: {title}\")\n",
    "        print(f\"   Total occurrences: {count}\")\n",
    "        print(f\"   Found in: {', '.join(sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all dataframes to common schema: id, title, abstract, library, authors, doi, journal, date\n",
    "\n",
    "# ACM\n",
    "acm_df[\"library\"] = \"acm\"\n",
    "acm_df = acm_df.rename(columns={\n",
    "    \"ID\": \"id\", \"title\": \"title\", \"abstract\": \"abstract\", \"author\": \"authors\",\n",
    "    \"doi\": \"doi\", \"journal\": \"journal\", \"year\": \"date\"\n",
    "})\n",
    "for col in [\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]:\n",
    "    if col not in acm_df.columns:\n",
    "        acm_df[col] = pd.NA\n",
    "acm_df = acm_df[[\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]]\n",
    "\n",
    "# Elsevier\n",
    "elsevier_df[\"library\"] = \"elsevier\"\n",
    "elsevier_df = elsevier_df.rename(columns={\n",
    "    \"ID\": \"id\", \"title\": \"title\", \"abstract\": \"abstract\", \"author\": \"authors\",\n",
    "    \"doi\": \"doi\", \"journal\": \"journal\", \"year\": \"date\"\n",
    "})\n",
    "if \"date\" in elsevier_df.columns:\n",
    "    elsevier_df[\"date\"] = elsevier_df[\"date\"].astype(str)\n",
    "for col in [\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]:\n",
    "    if col not in elsevier_df.columns:\n",
    "        elsevier_df[col] = pd.NA\n",
    "elsevier_df = elsevier_df[[\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]]\n",
    "\n",
    "# IEEE\n",
    "ieee_df[\"library\"] = \"ieee\"\n",
    "ieee_df = ieee_df.rename(columns={\n",
    "    \"ISBNs\": \"id\", \"Document Title\": \"title\", \"Abstract\": \"abstract\", \"Authors\": \"authors\",\n",
    "    \"DOI\": \"doi\", \"Publication Title\": \"journal\", \"Online Date\": \"date\"\n",
    "})\n",
    "for col in [\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]:\n",
    "    if col not in ieee_df.columns:\n",
    "        ieee_df[col] = pd.NA\n",
    "ieee_df = ieee_df[[\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]]\n",
    "\n",
    "# PubMed\n",
    "pubmed_df[\"library\"] = \"pubmed\"\n",
    "pubmed_df = pubmed_df.rename(columns={\n",
    "    \"PMID\": \"id\", \"Title\": \"title\", \"abstract\": \"abstract\", \"Authors\": \"authors\",\n",
    "    \"DOI\": \"doi\", \"Journal/Book\": \"journal\", \"Create Date\": \"date\"\n",
    "})\n",
    "for col in [\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]:\n",
    "    if col not in pubmed_df.columns:\n",
    "        pubmed_df[col] = pd.NA\n",
    "pubmed_df = pubmed_df[[\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]]\n",
    "\n",
    "# Springer - USE ACTUAL COLUMN NAMES from the parquet file\n",
    "springer_df[\"library\"] = \"springer\"\n",
    "springer_df = springer_df.rename(columns={\n",
    "    \"Item Title\": \"title\", \"Abstract\": \"abstract\", \"Authors\": \"authors\",\n",
    "    \"Item DOI\": \"doi\", \"Publication Title\": \"journal\", \"Publication Year\": \"date\"\n",
    "})\n",
    "# Add ID if missing\n",
    "if \"id\" not in springer_df.columns:\n",
    "    springer_df[\"id\"] = range(len(springer_df))\n",
    "if \"date\" in springer_df.columns:\n",
    "    springer_df[\"date\"] = springer_df[\"date\"].astype(str)\n",
    "for col in [\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]:\n",
    "    if col not in springer_df.columns:\n",
    "        springer_df[col] = pd.NA\n",
    "springer_df = springer_df[[\"id\", \"title\", \"abstract\", \"library\", \"authors\", \"doi\", \"journal\", \"date\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ACM records after normalization: {len(acm_df)}\")\n",
    "print(f\"IEEE records after normalization: {len(ieee_df)}\")\n",
    "print(f\"PubMed records after normalization: {len(pubmed_df)}\")\n",
    "print(f\"Elsevier records after normalization: {len(elsevier_df)}\")\n",
    "print(f\"Springer records after normalization: {len(springer_df)}\")\n",
    "print(f\"\\nSpringer columns: {springer_df.columns.tolist()}\")\n",
    "print(f\"Springer library value counts:\\n{springer_df['library'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d754899",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([acm_df, elsevier_df, ieee_df, pubmed_df, springer_df], ignore_index=True)\n",
    "print(len(combined_df))\n",
    "combined_df\n",
    "# print group count by date\n",
    "date_counts = combined_df['date'].value_counts().sort_index()\n",
    "print(\"Count of papers by date:\")\n",
    "date_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Library counts immediately after concat:\")\n",
    "print(combined_df['library'].value_counts())\n",
    "print(f\"Springer count: {(combined_df['library'] == 'springer').sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72389185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check library counts BEFORE duplicate removal\n",
    "print(\"Library counts BEFORE duplicate removal:\")\n",
    "print(combined_df['library'].value_counts())\n",
    "print(f\"\\nTotal records: {len(combined_df)}\\n\")\n",
    "\n",
    "# create a normalized comparison key\n",
    "combined_df['title_norm'] = combined_df['title'].str.lower().str.strip()\n",
    "\n",
    "# group and filter to only duplicated titles\n",
    "dupes = combined_df[combined_df.duplicated('title_norm', keep=False)]\n",
    "\n",
    "# now show which libraries the duplicates belong to\n",
    "result = dupes.groupby('title_norm')['library'].unique().reset_index()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP DUPLICATES\n",
    "combined_df['title_norm'] = combined_df['title'].str.lower().str.strip()\n",
    "\n",
    "# find duplicated keys\n",
    "dupe_keys = combined_df['title_norm'][combined_df['title_norm'].duplicated(keep=False)]\n",
    "\n",
    "# remove every row with those keys\n",
    "combined_df = combined_df[~combined_df['title_norm'].isin(dupe_keys)]\n",
    "\n",
    "combined_df = combined_df.drop(columns='title_norm')\n",
    "print(f\"Library counts AFTER duplicate removal: {len(combined_df)}\")\n",
    "# remove journal is medRxiv\n",
    "combined_df = combined_df[combined_df['journal'] != 'medRxiv']\n",
    "print(f\"Library counts AFTER removing medRxiv: {len(combined_df)}\")\n",
    "combined_df\n",
    "print(f\"Amount of papers after duplication removal: {len(combined_df)}\")\n",
    "\n",
    "# read my cache\n",
    "combined_df = pd.read_parquet(\"dataset/combined_filtered.parquet\")\n",
    "combined_df\n",
    "# save as csv\n",
    "# sort by date_crossreq\n",
    "\n",
    "combined_df = combined_df.sort_values(by='date_crossreq')\n",
    "\n",
    "print(f\"current date counts: {len(combined_df)}\")\n",
    "\n",
    "# filter to only papers with date between 2023 and 2025Q3\n",
    "combined_df = combined_df[\n",
    "    (combined_df['date_crossreq'] >= '2023-01-01') &\n",
    "    (combined_df['date_crossreq'] <= '2025-09-30')\n",
    "]\n",
    "# filter date is empty\n",
    "combined_df = combined_df[combined_df['date_crossreq'].notna()]\n",
    "print(f\"after filter date current date counts: {len(combined_df)}\")\n",
    "combined_df.to_csv(\"dataset/combined_filtered.csv\", index=False)\n",
    "# remove date which is not in 2023-2025Q3\n",
    "combined_df.to_parquet(\"dataset/combined_filtered2.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# caculate group count by library, and draw a bar chart\n",
    "library_counts = combined_df['library'].value_counts()\n",
    "print(library_counts)\n",
    "library_counts.plot(kind='bar', color=\"darkblue\", title='Number of Records by Library in scientific research')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Combine all abstracts into one string\n",
    "all_abstracts = ' '.join(combined_df['abstract'].dropna().astype(str))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load stop words\n",
    "my_stop_words = set(stopwords.words('english'))\n",
    "# remove words like \"using\",\"there\"\n",
    "custom_stops = {\"using\", \"there\", \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\",\"also\",\"however\",\"this\",\"that\",\"these\",\"those\"}\n",
    "stop_words = my_stop_words.union(custom_stops)\n",
    "\n",
    "# Create wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white',stopwords=my_stop_words).generate(all_abstracts)\n",
    "\n",
    "# Display\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.title('Wordcloud of Abstracts for scientific research papers')\n",
    "# plt.tight_layout(pad=0)\n",
    "# this image doesn't need to be shown in the paper, so we can comment it out,please read wordcloud_generate_general.py for details\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by journal and order by count descending\n",
    "top15journal = combined_df.groupby('journal').size().sort_values(ascending=False).head(15)\n",
    "# plot bar chart\n",
    "top15journal.plot(kind='bar', color=\"darkblue\", title='Top 15 Journals by Number of Publications in scientific research')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf00ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "custom_regex_dict = {\n",
    "    'chatgpt': re.compile(r'\\b(chat[\\s\\-\\.]*gpt)\\b|\\b(gpt)\\b', re.IGNORECASE),\n",
    "    'gpt-4': re.compile(r'\\b(gpt[\\s\\-\\.]*4(o|\\s*turbo)?)\\b', re.IGNORECASE),\n",
    "    'gpt-3.5': re.compile(r'\\b(gpt[\\s\\-\\.]*3[\\s\\-\\.]*5)\\b', re.IGNORECASE),\n",
    "    'gpt-5': re.compile(r'\\b(gpt[\\s\\-\\.]*5)\\b', re.IGNORECASE), \n",
    "    'gpt-3': re.compile(r'\\b(gpt[\\s\\-\\.]*3)\\b', re.IGNORECASE),\n",
    "    'gemini': re.compile(r'\\b(gemini)\\b', re.IGNORECASE),\n",
    "    'deepseek': re.compile(r'\\b(deepseek)\\b', re.IGNORECASE),\n",
    "    'llama': re.compile(r'\\b(llama)\\b', re.IGNORECASE),\n",
    "    'claude': re.compile(\n",
    "        r'\\b(claude)[\\s\\-\\.]*(\\(anthropic\\)|\\s*anthropic)?\\b|\\b(anthropic)\\b', \n",
    "        re.IGNORECASE\n",
    "    ),\n",
    "    'mistral': re.compile(\n",
    "    # Matches: 'mistral', 'mistral (le chat)', 'mistral le-chat'\n",
    "    r'\\b(mistral)[\\s\\-\\.]*(\\(le[\\s\\-\\.]*chat\\)|\\s*le[\\s\\-\\.]*chat)?\\b'\n",
    "    # OR Matches: 'le chat', 'le-chat', 'lechat' (requires 'le' and 'chat' together)\n",
    "    r'|\\b(le[\\s\\-\\.]*chat)\\b', \n",
    "    re.IGNORECASE\n",
    "    ),\n",
    "    'medgemma': re.compile(r'\\b(med[\\s\\-\\.]*gemma)\\b', re.IGNORECASE),\n",
    "    'gemma': re.compile(r'\\b(gemma)\\b', re.IGNORECASE),\n",
    "    'med-palm': re.compile(r'\\b(med[\\s\\-\\.]*palm)\\b', re.IGNORECASE), \n",
    "    'bert': re.compile(r'\\b(bert)\\b', re.IGNORECASE),\n",
    "    'qwen': re.compile(r'\\b(qwen)\\b', re.IGNORECASE),\n",
    "    'falcon': re.compile(r'\\b(falcon)\\b', re.IGNORECASE),\n",
    "    'phi': re.compile(r'\\b(phi)\\b', re.IGNORECASE),\n",
    "    'pubmedbert': re.compile(r'\\b(pubmedbert)\\b', re.IGNORECASE),\n",
    "}\n",
    "model_counts = {}\n",
    "\n",
    "# Loop through each model and its regex\n",
    "for model, regex in custom_regex_dict.items():\n",
    "    count = combined_df[\"abstract\"].str.count(regex).sum()\n",
    "    model_counts[model] = int(count)\n",
    "\n",
    "model_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = [\"performance\", \"method\", \"patient evaluated\", \"quality research\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7becad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_llms(text, regex_dict):\n",
    "    \"\"\"Finds all unique canonical LLMs that match a pattern in the text.\"\"\"\n",
    "    found_llms = []\n",
    "    for llm_name, pattern in regex_dict.items():\n",
    "        if pattern.search(text):\n",
    "            found_llms.append(llm_name)\n",
    "    return list(set(found_llms))\n",
    "\n",
    "def find_topics(text, keywords):\n",
    "    \"\"\"Finds all unique keywords that appear in the text.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    found_topics = []\n",
    "    for keyword in keywords:\n",
    "        # Simple substring search is sufficient since the topics are single words\n",
    "        if keyword in text_lower:\n",
    "            found_topics.append(keyword)\n",
    "    return list(set(found_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the functions to the DataFrame\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "df = combined_df.copy()\n",
    "df['Found_LLMs'] = df['abstract'].apply(\n",
    "    lambda x: find_llms(x, custom_regex_dict) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "# Find Topics\n",
    "df['Found_Topics'] = df['abstract'].apply(\n",
    "    lambda x: find_topics(x, topic_keywords) if pd.notna(x) else []\n",
    ")\n",
    "# Define Matrix Axes\n",
    "llm_labels = list(custom_regex_dict.keys())\n",
    "topic_labels = topic_keywords\n",
    "\n",
    "# Initialize the Occurrence Matrix\n",
    "occurrence_matrix = pd.DataFrame(\n",
    "    np.zeros((len(topic_labels), len(llm_labels)), dtype=int),\n",
    "    index=topic_labels,\n",
    "    columns=llm_labels\n",
    ")\n",
    "\n",
    "# Populate the occurrence matrix\n",
    "for _, row in df.iterrows():\n",
    "    llms_in_doc = row['Found_LLMs']\n",
    "    topics_in_doc = row['Found_Topics']\n",
    "    \n",
    "    # Iterate over every combination of LLM and Topic found in the document\n",
    "    for topic in topics_in_doc:\n",
    "        for llm in llms_in_doc:\n",
    "            # Increment the count where the topic and LLM co-occur\n",
    "            occurrence_matrix.loc[topic, llm] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    occurrence_matrix,\n",
    "    annot=True,        # Show the count in each cell\n",
    "    cmap=\"Blues\", # Reversed color map for better contrast\n",
    "    linewidths=.5,     \n",
    "    linecolor='black',\n",
    "    fmt='d' # forces integer display\n",
    ")\n",
    "plt.title('Co-occurrence Matrix: LLMs vs. Application Topics in scientific research', fontsize=16)\n",
    "plt.xlabel('Language Model', fontsize=14)\n",
    "plt.ylabel('Application Topic', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84412d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = combined_df.copy()\n",
    "start = pd.Timestamp('2023-01-01')\n",
    "cutoff = pd.Timestamp('2025-09-30')\n",
    "\n",
    "df[\"date_standard\"] = pd.to_datetime(df[\"date\"], errors='coerce', dayfirst=True)\n",
    "df = df[(df['date_standard'] >= start) & (df['date_standard'] <= cutoff)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629331c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure very paper is fouchs on LLMs & Health on scientific research, by using regular expression to filter the title and abstract\n",
    "llm_pattern = re.compile(\n",
    "    r'\\b(chat[\\s\\-\\.]*gpt|gpt[\\s\\-\\.]*4(o|\\s*turbo)?|gpt[\\s\\-\\.]*3[\\s\\-\\.]*5|gpt[\\s\\-\\.]*5|gpt[\\s\\-\\.]*3|gemini|deepseek|llama|claude[\\s\\-\\.]*(\\(anthropic\\)|\\s*anthropic)?|mistral[\\s\\-\\.]*(\\(le[\\s\\-\\.]*chat\\)|\\s*le[\\s\\-\\.]*chat)?|le[\\s\\-\\.]*chat|med[\\s\\-\\.]*gemma|gemma|med[\\s\\-\\.]*palm|bert|qwen|falcon|phi|pubmedbert)\\b', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "health_pattern = re.compile(\n",
    "    r'\\b(health|healthcare|medicine|medical|clinical|patient|diagnosis|treatment|disease|symptom|hospital|doctor|nurse|pharmacy|biomedical|public health|epidemiology|mental health|health informatics|health data|health outcomes|health services|health policy|health management|health economics|health education|health promotion|health disparities|health equity|health behavior|health communication|health literacy|health intervention|health monitoring|health assessment|health research|health innovation|health technology|health system|healthcare delivery|healthcare management|healthcare policy|healthcare economics|healthcare education|healthcare promotion|healthcare disparities|healthcare equity|healthcare behavior|healthcare communication|healthcare literacy|healthcare intervention|healthcare monitoring|healthcare assessment|healthcare research|healthcare innovation|healthcare technology|healthcare system)\\b', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "scientific_research_pattern = re.compile(\n",
    "    r'\\b(scientific research|research|study|analysis|evaluation|investigation|experiment|clinical trial|systematic review|meta-analysis|case study|longitudinal study|cross-sectional study|observational study|randomized controlled trial|cohort study|qualitative research|quantitative research)\\b', \n",
    "    re.IGNORECASE\n",
    ")\n",
    "def is_relevant_paper(title, abstract):\n",
    "    text = f\"{title} {abstract}\"\n",
    "    return bool(llm_pattern.search(text)) and bool(health_pattern.search(text)) and bool(scientific_research_pattern.search(text))\n",
    "# Apply the relevance filter\n",
    "df['is_relevant'] = df.apply(lambda row: is_relevant_paper(row['title'], row['abstract']), axis=1)\n",
    "# Filter to only relevant papers\n",
    "relevant_df = df[df['is_relevant']]\n",
    "print(f\"Number of papers after relevance filtering: {len(relevant_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates from the raw combined_df['date'] to avoid filtered-state issues\n",
    "import re\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "\n",
    "\n",
    "\n",
    "# attach to a fresh copy and restrict to window\n",
    "ndf = df.copy()\n",
    "ndf['date_standard'] = ndf['date_crossreq']\n",
    "start = pd.Timestamp('2023-01-01')\n",
    "end = pd.Timestamp('2025-09-30')\n",
    "ndf = ndf[(ndf['date_standard'] >= start) & (ndf['date_standard'] <= end)]\n",
    "\n",
    "# quarterly\n",
    "ndf['year_quarter'] = ndf['date_standard'].dt.to_period('Q')\n",
    "quarter_counts = ndf['year_quarter'].value_counts().sort_index()\n",
    "full_q = pd.period_range(start='2023Q1', end='2025Q3', freq='Q')\n",
    "quarter_counts = quarter_counts.reindex(full_q, fill_value=0)\n",
    "\n",
    "# show sample parsed rows that have non-January months\n",
    "sample_non_jan = ndf.loc[ndf['date_standard'].dt.month != 1, ['date','date_standard']].head(30)\n",
    "print('\\nSample non-January parsed rows:')\n",
    "print(sample_non_jan)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar([str(q) for q in full_q], quarter_counts.values, color='darkblue')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Scientific Research Paper Counts by Quarter')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nQuarter counts:')\n",
    "print(quarter_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: inspect rows with month names and how dateutil parses them\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "months_regex = r\"(?i)\\b(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\b\"\n",
    "\n",
    "mask = combined_df['date'].astype(str).str.contains(months_regex, regex=True, na=False)\n",
    "sample = combined_df.loc[mask, ['library','date']].head(100).copy()\n",
    "\n",
    "def try_parse(x):\n",
    "    txt = str(x)\n",
    "    try:\n",
    "        return pd.Timestamp(dateutil_parse(txt, dayfirst=True, fuzzy=True))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.Timestamp(dateutil_parse(txt, dayfirst=False, fuzzy=True))\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "\n",
    "sample['parsed'] = sample['date'].apply(try_parse)\n",
    "print('Sample parsed results (month-name rows):')\n",
    "print(sample.to_string())\n",
    "\n",
    "# Show count of parsed months for month-name rows\n",
    "print('\\nParsed month value counts for these rows:')\n",
    "print(sample['parsed'].dt.month.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: inspect parsed months and sample raw date strings\n",
    "print('Parsed date_standard month counts:')\n",
    "print(df['date_standard'].dt.month.value_counts().sort_index())\n",
    "\n",
    "print('\\nSample raw -> parsed (first 50 non-null parsed):')\n",
    "sample = df.loc[df['date_standard'].notna(), ['date','date_standard']].head(50)\n",
    "for i, row in sample.iterrows():\n",
    "    print(row['date'], '->', row['date_standard'])\n",
    "\n",
    "print('\\nExamples where month != 1 (should show Q2-Q4 entries):')\n",
    "examples = df.loc[df['date_standard'].notna() & (df['date_standard'].dt.month != 1), ['date','date_standard']].head(50)\n",
    "print(len(examples))\n",
    "for i, row in examples.iterrows():\n",
    "    print(row['date'], '->', row['date_standard'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156871f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search combined_df['date'] for month names / month-like strings\n",
    "months_regex = r\"(?i)\\b(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\b\"\n",
    "mask = combined_df['date'].astype(str).str.contains(months_regex, regex=True, na=False)\n",
    "print('Records with month names in `date`:', mask.sum())\n",
    "print('\\nSample rows with month in date:')\n",
    "print(combined_df.loc[mask, ['library','date']].head(40))\n",
    "\n",
    "# Also check for formats like '2025-September' or '2024-09'\n",
    "mask2 = combined_df['date'].astype(str).str.contains(r'\\d{4}[-_/]\\s*[A-Za-z]+|\\d{4}[-_/]\\s*\\d{1,2}', regex=True, na=False)\n",
    "print('\\nRecords matching year-month patterns:', mask2.sum())\n",
    "print(combined_df.loc[mask2, ['library','date']].head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df\n",
    "\n",
    "print(f\"Total papers before filtering: {len(combined_df)}\")\n",
    "\n",
    "# 1. define regular expression for filtering section 3.7\n",
    "keywords = [\n",
    "    \"automated systematic review\", \"automated review\", \"evidence synthesis\",\n",
    "    \"hypothesis generation\", \"scientific discovery\", \"AI scientist\",\n",
    "    \"CRISPR-GPT\", \"FutureHouse\", \"Laser AI\", \"gene editing\",\n",
    "    \"experimental design\", \"troubleshooting protocols\", \"autonomous agents\",\"review\", \"systematic review\", \"literature review\", \"meta-analysis\", \"research synthesis\", \"research evaluation\", \"research assessment\", \"research quality\", \"research performance\", \"patient evaluated\", \"quality research\"\n",
    "]\n",
    "\n",
    "# 2. build combined regex pattern\n",
    "regex_pattern = '|'.join(keywords)\n",
    "\n",
    "# 3. filter abstracts using the regex pattern\n",
    "# na=False filters out NaN abstracts\n",
    "gaosong_papers = combined_df[combined_df['abstract'].str.contains(regex_pattern, case=False, na=False)]\n",
    "\n",
    "# 4. if needed, further filter for LLM mentions\n",
    "# gaosong_papers = gaosong_papers[gaosong_papers['abstract'].str.contains(\"LLM|Large Language Model\", case=False, na=False)]\n",
    "print(f\"papers filter abstracts using the regex pattern: {len(gaosong_papers)}\")\n",
    "print(f\"筛选出的论文数量: {len(gaosong_papers)}\")\n",
    "gaosong_papers\n",
    "\n",
    "# filter papers that abstract contains \"CRISPR-GPT\"\n",
    "crispr_gpt_papers = combined_df[combined_df['abstract'].str.contains(\"CRISPR\", case=False, na=False)]\n",
    "print(f\"筛选出的CRISPR-GPT相关论文数量: {len(crispr_gpt_papers)}\")\n",
    "# only print titile and abstract(no truncate) for crispr_gpt_papers\n",
    "\n",
    "# print title and abstract without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "crispr_gpt_papers.to_csv(\"crispr_gpt_papers.csv\", index=False)\n",
    "# reset display option\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
